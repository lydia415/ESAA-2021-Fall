{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBT1nQxkCX4g"
   },
   "source": [
    "# 0913 세션 데이콘 필사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 1051,
     "status": "ok",
     "timestamp": 1631532763998,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "bdeUrZ8k4wKf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('train.csv', encoding = 'utf-8')\n",
    "test = pd.read_csv('test_x.csv', encoding = 'utf-8')\n",
    "sample_submission = pd.read_csv('sample_submission.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jO3XHlpCvPuL"
   },
   "source": [
    "# 2 NLP 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35yIJWpRrYue"
   },
   "source": [
    "1. 형태소 분석 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3152,
     "status": "ok",
     "timestamp": 1631532767148,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "4Mzz8PsorCoB",
    "outputId": "1c3e22ac-bf7e-4560-da77-e72db6db909f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (0.5.2)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: colorama in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from konlpy) (4.6.1)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from konlpy) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from konlpy) (1.19.2)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from tweepy>=3.7.0->konlpy) (2.24.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/hyeminchon/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1631532767148,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "tBzn_ZOHqk_r",
    "outputId": "c18b1f1e-6757-46e8-e5a9-9005af45a78b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 단위로 문장 분리\n",
      "----------------------\n",
      "['데이', '콘', '에서', '다양', '하', 'ㄴ', '컴피티션', '을', '즐기', '면서', '실력', '있', '는', '데이터', '분석가', '로', '성장', '하', '세요', '!!', '.']\n",
      " \n",
      "문장에서 명사 추출\n",
      "----------------------\n",
      "['데이', '데이콘', '콘', '다양', '컴피티션', '실력', '데이터', '분석가', '성장']\n",
      " \n",
      "품사 태킹(PoS)\n",
      "----------------------\n",
      "[('데이', 'NNG'), ('콘', 'NNG'), ('에서', 'JKM'), ('다양', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('컴피티션', 'UN'), ('을', 'JKO'), ('즐기', 'VV'), ('면서', 'ECE'), ('실력', 'NNG'), ('있', 'VV'), ('는', 'ETD'), ('데이터', 'NNG'), ('분석가', 'NNG'), ('로', 'JKM'), ('성장', 'NNG'), ('하', 'XSV'), ('세요', 'EFN'), ('!!', 'SW'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
    "\n",
    "print(\"형태소 단위로 문장 분리\")\n",
    "print(\"----------------------\")\n",
    "print(kkma.morphs(sentence))\n",
    "print(\" \")\n",
    "print(\"문장에서 명사 추출\")\n",
    "print(\"----------------------\")\n",
    "print(kkma.nouns(sentence))\n",
    "print(\" \")\n",
    "print(\"품사 태킹(PoS)\")\n",
    "print(\"----------------------\")\n",
    "print(kkma.pos(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1631532767148,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "uHY_HCUorBjh",
    "outputId": "9f8f686f-b8b5-4513-cff1-6e717218782e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 단위로 문장 분리\n",
      "----------------------\n",
      "['데', '이콘', '에서', '다양한', '컴피티션', '을', '즐기면서', '실력', '있는', '데이터', '분석', '가로', '성장하세요', '!!.']\n",
      " \n",
      "문장에서 명사 추출\n",
      "----------------------\n",
      "['데', '이콘', '컴피티션', '실력', '데이터', '분석', '가로']\n",
      " \n",
      "품사 태킹(PoS)\n",
      "----------------------\n",
      "[('데', 'Noun'), ('이콘', 'Noun'), ('에서', 'Josa'), ('다양한', 'Adjective'), ('컴피티션', 'Noun'), ('을', 'Josa'), ('즐기면서', 'Verb'), ('실력', 'Noun'), ('있는', 'Adjective'), ('데이터', 'Noun'), ('분석', 'Noun'), ('가로', 'Noun'), ('성장하세요', 'Adjective'), ('!!.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "Okt = Okt()\n",
    "\n",
    "sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
    "\n",
    "print(\"형태소 단위로 문장 분리\")\n",
    "print(\"----------------------\")\n",
    "print(Okt.morphs(sentence))\n",
    "print(\" \")\n",
    "print(\"문장에서 명사 추출\")\n",
    "print(\"----------------------\")\n",
    "print(Okt.nouns(sentence))\n",
    "print(\" \")\n",
    "print(\"품사 태킹(PoS)\")\n",
    "print(\"----------------------\")\n",
    "print(Okt.pos(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3oFx0_d7bdT"
   },
   "source": [
    "from konlpy.tag import Mecab \n",
    "Mecab  = Mecab ()\n",
    "\n",
    "sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
    "\n",
    "print(\"형태소 단위로 문장 분리\")\n",
    "print(\"----------------------\")\n",
    "print(Mecab .morphs(sentence))\n",
    "print(\" \")\n",
    "print(\"문장에서 명사 추출\")\n",
    "print(\"----------------------\")\n",
    "print(Mecab .nouns(sentence))\n",
    "print(\" \")\n",
    "print(\"품사 태킹(PoS)\")\n",
    "print(\"----------------------\")\n",
    "print(Mecab .pos(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM0hFYWLreIt"
   },
   "source": [
    "2. 표제어 추출 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1631532767671,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "if7sPTr6rONY",
    "outputId": "fdf9652a-b262-40c3-ced8-86fbcc00657e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태킹(PoS)\n",
      "----------------------\n",
      "[('성장', 'NNG'), ('하', 'XSV'), ('었', 'EPT'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "sentence = '성장했었다.'\n",
    "\n",
    "print(\"품사 태킹(PoS)\")\n",
    "print(\"----------------------\")\n",
    "print(kkma.pos(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1631532767671,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "hDpyD6s3rOK3",
    "outputId": "6a6a3c93-7d7e-4dcb-b233-dde2857c6563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태킹(PoS)\n",
      "----------------------\n",
      "[('성장', 'NNG'), ('하', 'XSV'), ('였', 'EPT'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "sentence = '성장하였었다.'\n",
    "\n",
    "print(\"품사 태킹(PoS)\")\n",
    "print(\"----------------------\")\n",
    "print(kkma.pos(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMwedqDbrsff"
   },
   "source": [
    "3. 불용어 제거 Stopwords removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1631532768719,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "nFloDArdrren"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Okt\n",
    "tokenizer = Okt()\n",
    "def text_preprocessing(text,tokenizer):\n",
    "    \n",
    "    stopwords = ['을', '를', '이', '가', '은', '는']\n",
    "    \n",
    "    txt = re.sub('[^가-힣a-z]', ' ', text)\n",
    "    token = tokenizer.morphs(txt)\n",
    "    token = [t for t in token if t not in stopwords]\n",
    "        \n",
    "    return token\n",
    "\n",
    "ex_text = \"이번에 새롭게 개봉한 영화의 배우들은 모두 훌륭한 연기력과 아름다운 목소리를 갖고 있어!!\"\n",
    "example_pre= text_preprocessing(ex_text,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1631532768720,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "l1Grt_vRrrca",
    "outputId": "ddbe6d16-522f-4e2d-fcc4-f4ed27dd01c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이번', '에', '새롭게', '개봉', '한', '영화', '의', '배우', '들', '모두', '훌륭한', '연기력', '과', '아름다운', '목소리', '갖고', '있어']\n"
     ]
    }
   ],
   "source": [
    "print(example_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_Bqg824sGgH"
   },
   "source": [
    "4. 대회 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "executionInfo": {
     "elapsed": 32424,
     "status": "error",
     "timestamp": 1631532916563,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "tIL5O8A1rOEx",
    "outputId": "1a7d6b8d-82bf-48ba-eaa0-ecccf51ea757"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text_list):\n",
    "    \n",
    "    stopwords = ['을', '를', '이', '가', '은', '는', 'null'] #불용어 설정\n",
    "    tokenizer = Okt() #형태소 분석기 \n",
    "    token_list = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        txt = re.sub('[^가-힣a-z]', ' ', text) #한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
    "        token = tokenizer.morphs(txt) #형태소 분석\n",
    "        token = [t for t in token if t not in stopwords or type(t) != float] #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
    "        token_list.append(token)\n",
    "        \n",
    "    return token_list, tokenizer\n",
    "\n",
    "train['new_article'], okt = text_preprocessing(train['text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPcBn6IUvIIs"
   },
   "source": [
    "#3 Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1631532920758,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "_grz1ZXIrN7T",
    "outputId": "ba8e83e5-289a-4c51-fae9-bb5cac720595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 단위로 문장 분리\n",
      "----------------------\n",
      "['자연어 처리 는 정말 정말 즐거워', '즐거운 자연어 처리 다 같이 해보자']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "Okt = Okt()\n",
    "\n",
    "sentences = ['자연어 처리는 정말 정말 즐거워.', '즐거운 자연어 처리 다같이 해보자.']\n",
    "tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub('[^가-힣a-z]', ' ', sentence) #간단한 전처리\n",
    "    token = (Okt.morphs(sentence)) #형태소 분석기를 이용햔 토큰 나누기\n",
    "    tokens.append(' '.join(token))\n",
    "\n",
    "print(\"형태소 단위로 문장 분리\")\n",
    "print(\"----------------------\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbquekzfvbhu"
   },
   "source": [
    "1. One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5769,
     "status": "ok",
     "timestamp": 1631532930195,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "I4_Szm9OqreF",
    "outputId": "02f33fd2-29cc-424e-9042-e082412cbc0e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-6bfd5d59041f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(tokens)\n",
    "print(\"각 토큰에게 고유의 정수 부여\")\n",
    "print(\"----------------------\")\n",
    "print(t.word_index) \n",
    "print(\" \")\n",
    "\n",
    "s1=t.texts_to_sequences(tokens)[0] \n",
    "print(\"부여된 정수로 표시된 문장1\")\n",
    "print(\"----------------------\")\n",
    "print(s1)\n",
    "print(\" \")\n",
    "\n",
    "s2=t.texts_to_sequences(tokens)[1]\n",
    "print(\"부여된 정수로 표시된 문장2\")\n",
    "print(\"----------------------\")\n",
    "print(s2)\n",
    "print(\" \")\n",
    "\n",
    "s1_one_hot = to_categorical(s1)\n",
    "print(\"문장1의 one-hot-encoding\")\n",
    "print(\"----------------------\")\n",
    "print(s1_one_hot)\n",
    "print(\" \")\n",
    "\n",
    "s2_one_hot = to_categorical(s2)\n",
    "print(\"문장2의 one-hot-encoding\")\n",
    "print(\"----------------------\")\n",
    "print(s2_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-eNhWxXvpCB"
   },
   "source": [
    "2. Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1631532936488,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "8Zo-ih0Evn-2",
    "outputId": "83542631-fcc3-4919-daa9-58a0421a6c2f"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(tokens) #여러 개의 문장을 넣어줘야 작동합니다!!\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVkOJIEWvvz5"
   },
   "source": [
    "3. TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1631532940158,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "Fhso2MrPvaj0",
    "outputId": "a131b7c4-241c-41b6-e457-0e8411826b23"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=0)\n",
    "tfidf_vectorizer = tfidf.fit_transform(tokens) \n",
    "\n",
    "#tf-idf dictionary    \n",
    "tfidf_dict = tfidf.get_feature_names()\n",
    "print(tfidf_dict)\n",
    "print(tfidf_vectorizer.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIySbkoZ8VsF"
   },
   "source": [
    "4. Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7866,
     "status": "ok",
     "timestamp": 1631532983673,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "RKHFht23vamM",
    "outputId": "d45496b6-569f-4a9f-a8f1-1dbfa9f729d7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def text2sequence(train_text, max_len=100):\n",
    "    \n",
    "    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n",
    "    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n",
    "    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n",
    "    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n",
    "    print('vocab_size : ', vocab_size)\n",
    "    X_train = pad_sequences(train_X_seq, maxlen = max_len) #설정한 문장의 최대 길이만큼 padding\n",
    "    \n",
    "    return X_train, vocab_size, tokenizer\n",
    "\n",
    "train_X, vocab_size, vectorizer = text2sequence(train['text'], max_len = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G32Xncb8cAV"
   },
   "source": [
    "# Facebook의 fasttext를 이용한 text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1642,
     "status": "ok",
     "timestamp": 1631534447697,
     "user": {
      "displayName": "­전혜민(사회과학대학 사회복지학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14600815532759021620"
     },
     "user_tz": -540
    },
    "id": "yInD7EU6vao4"
   },
   "outputs": [],
   "source": [
    "from gensim.models import fasttext\n",
    "\n",
    "#  디폴트로 돌리면 0.56\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file = open('fasttexttrain.txt','w+')\n",
    "for i in train.index:\n",
    "    line = '__label__' + str(train['author'][i])+' '+train['text'][i]\n",
    "    file.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOeSwkbnvarZ"
   },
   "outputs": [],
   "source": [
    "text_clf_model = fasttext.train_supervised('fasttexttrain.txt', epoch=30, minCount=2, maxn=10, verbose=0)\n",
    "print(text_clf_model.words)\n",
    "print(text_clf_model.labels)\n",
    "\n",
    "reuslt = text_clf_model.predict(\"He was almost choking. There was so much, so much he wanted to say, but strange exclamations were all that came from his lips. The Pole gazed fixedly at him, at the bundle of notes in his hand; looked at odin, and was in evident perplexity.\", k=5)\n",
    "print(reuslt)\n",
    "\n",
    "test = pd.read_csv('../input/fasttext/test_x.csv')\n",
    "submission = pd.read_csv('../input/fasttext/sample_submission.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utb8Gqizvatv"
   },
   "outputs": [],
   "source": [
    "for i in test.index:\n",
    "    lable, proba = text_clf_model.predict(test['text'][i], k=5)\n",
    "    for la, pr in zip(lable, proba):\n",
    "        if '__label__0' == la:\n",
    "            submission.loc[i, '0'] = pr\n",
    "        elif '__label__1' == la:\n",
    "            submission.loc[i, '1'] = pr\n",
    "        elif '__label__2' == la:\n",
    "            submission.loc[i, '2'] = pr\n",
    "        elif '__label__3' == la:\n",
    "            submission.loc[i, '3'] = pr\n",
    "        elif '__label__4' == la:\n",
    "            submission.loc[i, '4'] = pr\n",
    "    # submission.loc[i, '0'] = proba[lable.loc('__label__0')]\n",
    "    # submission.loc[i, '1'] = proba[4]\n",
    "    # submission.loc[i, '2'] = proba[2]\n",
    "    # submission.loc[i, '3'] = proba[0]\n",
    "    # submission.loc[i, '4'] = proba[3]\n",
    "\n",
    "submission.to_csv('result5_fasttext.csv', index=False)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0927 데이콘 필사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Keras Embedding Layer (단어 사이의 관계를 반영하는 방법이 아닙니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length = max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
    "\n",
    "for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
    "    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
    "        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고\n",
    "        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n",
    "    else:\n",
    "        print(\"word2vec에 없는 단어입니다.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = dict()\n",
    "f = open('./glove.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = asarray(values[1:], dtype='float32')\n",
    "    glove[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
    "\n",
    "for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
    "    if word in glove: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
    "        embedding_vector = glove[word] #해당 토큰에 해당하는 vector를 불러오고\n",
    "        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n",
    "    else:\n",
    "        print(\"glove 없는 단어입니다.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "FastText = KeyedVectors.load_word2vec_format('./fasttext.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
    "\n",
    "for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
    "    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
    "        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고\n",
    "        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 전처리 + 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text_list):\n",
    "    \n",
    "    stopwords = ['을', '를', '이', '가', '은', '는', 'null']\n",
    "    tokenizer = Okt()\n",
    "    \n",
    "    for text in tqdm.tqdm(text_list):\n",
    "        txt = re.sub('[^가-힣a-z]', ' ', text.lower())\n",
    "        token = tokenizer.morphs(txt)\n",
    "        token = [t for t in token if t not in stopwords or type(t) != float]\n",
    "        \n",
    "    return token, tokenizer\n",
    "\n",
    "train['token'], okt = text_preprocessing(train['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2sequence(train_text, max_len=1000):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_text)\n",
    "    train_X_seq = tokenizer.texts_to_sequences(train_text)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('vocab_size : ', vocab_size)\n",
    "    X_train = pad_sequences(train_X_seq, maxlen = max_len)\n",
    "    return X_train, vocab_size, tokenizer\n",
    "\n",
    "train_y = train['info']\n",
    "train_X, vocab_size, vectorizer = text2sequence(train['token'], max_len = 100)\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "\n",
    "for index, word in enumerate(vocabulary):\n",
    "    if word in word2vec\n",
    "        embedding_vector = word2vec[word] \n",
    "        embedding_mxtrix[i] = embedding_vector \n",
    "    else:\n",
    "        print(\"word2vec에 없는 단어입니다.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(vocab_size, max_len=1000):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len)) #임베딩 가중치 적용 코드\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "0913 essa session.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vp": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "title_cell": "VisualPython",
   "title_sidebar": "VisualPython",
   "vpPosition": {
    "height": "calc(100% - 180px)",
    "right": "10px",
    "top": "110px",
    "width": "50%"
   },
   "vp_cell": false,
   "vp_section_display": true,
   "vp_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
